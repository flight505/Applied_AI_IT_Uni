{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"PROJECT_reinforcement_learning.ipynb","provenance":[],"authorship_tag":"ABX9TyOfKZZMw9OvQr7TwBuALqAe"},"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('AI_ITU': conda)"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"529a06fb4068a4b13268933b910005f265dccc230859ef1ec378adb85153fb8f"}},"cells":[{"cell_type":"markdown","source":["# In this project we will solve two simple environments using a Q-table and a Neural Network (Deep Q-learning)."],"metadata":{"id":"NGzC3uqmuKZB"}},{"cell_type":"markdown","source":["# Subproject 1\n","\n","Solve [`FrozenLake8x8-v0`](https://gym.openai.com/envs/FrozenLake8x8-v0/) using a Q-table.\n"],"metadata":{"id":"KYeKUsX8uXSF"}},{"cell_type":"markdown","source":["1. Import Necessary Packages:"],"metadata":{"id":"hGAOGNSWyncb"}},{"cell_type":"markdown","source":["\n","2. Instantiate the Environment and Agent"],"metadata":{"id":"V7KHXZDxys6J"}},{"cell_type":"markdown","source":["3. Set up the QTable:"],"metadata":{"id":"QMs2BVFZywAJ"}},{"cell_type":"markdown","source":["4. The Q-Learning algorithm training"],"metadata":{"id":"YHuDteJVy2_C"}},{"cell_type":"markdown","source":["5. Evaluate how well your agent performs\n","* Render output of one episode\n","* Give an average episode return"],"metadata":{"id":"Mm8oigYjzFTd"}},{"cell_type":"markdown","source":["## Step 1: Import libs"],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["# Import packages\n","import numpy as np\n","import gym\n","import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Visualization function\n","#%run Draw_FrozenLake.ipynb\n"],"outputs":[],"metadata":{"id":"wiXJPDzauAvV"}},{"cell_type":"markdown","source":["## Step 2: Initiate the environment and agent.\n","* add FrozenLake8x8-v0 environment.\n","\n","OpenAI Gym is a library composed of many environments that we can use to train our agents.\n","In our case we choose to use Frozen Lake.\n","\n"],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["#Create Gym\n","from gym import wrappers\n","#env = gym.make(\"FrozenLake-v0\")\n","env = gym.make(\"FrozenLake8x8-v0\")\n","env.render()"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Step 3: setup the Q-table\n","\n","\n","* Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the `action_size` and the `state_size`\n","* OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["action_size = env.action_space.n\n","print(\"Action size \", action_size)\n","\n","state_size = env.observation_space.n\n","print(\"State size \", state_size)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Action size  4\n","State size  64\n"]}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["qtable = np.zeros((state_size, action_size))\n","print(qtable)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## 4. The Q-Learning algorithm training\n","Here, we'll specify the hyperparameters\n"],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["total_episodes = 15000        # Total episodes\n","learning_rate = 0.8           # Learning rate\n","max_steps = 99                # Max steps per episode\n","gamma = 0.95                  # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability \n","decay_rate = 0.005             # Exponential decay rate for exploration prob"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Now we implement the Q learning algorithm: Q algo"],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["# List of rewards\n","rewards = []\n","\n","# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","    \n","    for step in range(max_steps):\n","        # 3. Choose an action a in the current world state (s)\n","        ## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","        \n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        \n","        total_rewards += reward\n","        \n","        # Our new state is state\n","        state = new_state\n","        \n","        # If done (if we're dead) : finish episode\n","        if done == True: \n","            break\n","        \n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n","    rewards.append(total_rewards)\n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Score over time: 0.0\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["## Step 5: Use our Q-table to play FrozenLake ! üëæ\n","After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n","By running this cell you can see our agent playing FrozenLake."],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["env.reset()\n","\n","for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        \n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","        \n","        new_state, reward, done, info = env.step(action)\n","        \n","        if done:\n","            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n","            env.render()\n","            \n","            # We print the number of step it took.\n","            print(\"Number of steps\", step)\n","            break\n","        state = new_state\n","env.close()"],"outputs":[{"output_type":"stream","name":"stdout","text":["****************************************************\n","EPISODE  0\n","****************************************************\n","EPISODE  1\n","****************************************************\n","EPISODE  2\n","****************************************************\n","EPISODE  3\n","****************************************************\n","EPISODE  4\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# Subproject 2\n","\n","Solve [MoonLander-v2](https://gym.openai.com/envs/LunarLander-v2/) using DQN."],"metadata":{"id":"w0yEuu22vVDK"}},{"cell_type":"markdown","source":["**1. Import Necessary Packages:**\n"],"metadata":{"id":"qWzbaAl3zlde"}},{"cell_type":"code","execution_count":31,"source":["\n","#Imports\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import tensorflow as tf\n","from tensorflow import keras\n","#from keras.models import Sequential\n","#from keras.layers import Dense\n","#from keras.optimizers import Adam\n","import random\n","from gym import wrappers\n","#from stable_baselines3.common.evaluation import evaluate_policy\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apijQg5Izmms","executionInfo":{"status":"ok","timestamp":1627370684295,"user_tz":-120,"elapsed":7146,"user":{"displayName":"ƒêorƒëe Grbiƒá","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMtdPDPC6XJoAlVJ9SyzF_WldAaS1YlXbiUOAReA=s64","userId":"17224412897437983949"}},"outputId":"3c4e6d29-5f06-40fe-8047-900366ee662b"}},{"cell_type":"markdown","source":["**2. Instantiate the Environment**"],"metadata":{"id":"tBMyGxsqzwCd"}},{"cell_type":"code","execution_count":32,"source":["model = DQN('MlpPolicy', 'LunarLander-v2', verbose=1, exploration_final_eps=0.1, target_update_interval=250)\n","\n","# Separate env for evaluation\n","\n","env = gym.make('LunarLander-v2')\n","env.seed(0)\n","print('State shape: ', env.observation_space.shape)\n","print('Number of Actions: ', env.action_space.n)"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'DQN' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-a6277c893512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_final_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_update_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Separate env for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DQN' is not defined"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9dsd3yAz7Jp","executionInfo":{"status":"ok","timestamp":1627370689208,"user_tz":-120,"elapsed":390,"user":{"displayName":"ƒêorƒëe Grbiƒá","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMtdPDPC6XJoAlVJ9SyzF_WldAaS1YlXbiUOAReA=s64","userId":"17224412897437983949"}},"outputId":"1d6fe8a5-9d2d-4c02-ff70-2523a3d6c12a"}},{"cell_type":"markdown","source":["**3. Implement and instantiate the agent**\n","\n"],"metadata":{"id":"Paf8yGHfz--m"}},{"cell_type":"code","execution_count":null,"source":["# Random Agent, before training\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n","\n","print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"],"outputs":[],"metadata":{"id":"kgmFtepK0G1d"}},{"cell_type":"markdown","source":["**4. Train the agent with DQN**\n","\n","4.1 Show the episode return plot\n","  \n","  - Is the agent learning to solve the task?\n","\n","4.2 Save the best model"],"metadata":{"id":"Pw_aBoa40KkO"}},{"cell_type":"code","execution_count":null,"source":["# Train the agent\n","model.learn(total_timesteps=int(1e5))\n","# Save the agent\n","model.save(\"dqn_lunar\")\n","del model  # delete trained model to demonstrate loading"],"outputs":[],"metadata":{"id":"wsG2JUqF0N_B"}},{"cell_type":"markdown","source":["**5. Load the model from the disk and run it in a loop**\n","- Hint: if you want to see the agent laning the Moon Lander, type `env.render()` after the `env.step()`.\n","- Do to Colab not cooperating with the Gym rendering, you might want to download the trained model and run this loop on you computer to visualise the behavior."],"metadata":{"id":"JSVgVSMp0OOV"}},{"cell_type":"markdown","source":["**Helper functions**"],"metadata":{"id":"bQz2t49p1JHG"}},{"cell_type":"code","execution_count":null,"source":["model = DQN.load(\"dqn_lunar\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Evaluate the trained agent\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n","\n","print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Save rendered images:"],"metadata":{"id":"9jhtc8jF1XB2"}},{"cell_type":"code","execution_count":null,"source":["import imageio\n","import numpy as np\n","\n","images = []\n","images.append(img)\n","img = model.env.render(mode='rgb_array')\n","\n","imageio.mimwrite('./moonlander.gif',\n","                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n","                fps=29)"],"outputs":[],"metadata":{"id":"Z_JgokDf1L0E"}},{"cell_type":"markdown","source":["Display saved .gif"],"metadata":{"id":"wBRNoQ4X1heu"}},{"cell_type":"code","execution_count":null,"source":["from pathlib import Path\n","gifPath = Path(\"./moonlander.gif\")\n","# Display GIF in Jupyter, CoLab, IPython\n","with open(gifPath,'rb') as f:\n","    display.Image(data=f.read(), format='png')"],"outputs":[],"metadata":{"id":"1J7b1mCL1km8"}}]}