{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PROJECT_reinforcement_learning.ipynb","provenance":[],"authorship_tag":"ABX9TyOfKZZMw9OvQr7TwBuALqAe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NGzC3uqmuKZB"},"source":["# In this project we will solve two simple environments using a Q-table and a Neural Network (Deep Q-learning)."]},{"cell_type":"markdown","metadata":{"id":"KYeKUsX8uXSF"},"source":["# Subproject 1\n","\n","Solve [`FrozenLake8x8-v0`](https://gym.openai.com/envs/FrozenLake8x8-v0/) using a Q-table.\n"]},{"cell_type":"markdown","metadata":{"id":"hGAOGNSWyncb"},"source":["1. Import Necessary Packages:"]},{"cell_type":"markdown","metadata":{"id":"V7KHXZDxys6J"},"source":["\n","2. Instantiate the Environment and Agent"]},{"cell_type":"markdown","metadata":{"id":"QMs2BVFZywAJ"},"source":["3. Set up the QTable:"]},{"cell_type":"markdown","metadata":{"id":"YHuDteJVy2_C"},"source":["4. The Q-Learning algorithm training"]},{"cell_type":"markdown","metadata":{"id":"Mm8oigYjzFTd"},"source":["5. Evaluate how well your agent performs\n","* Render output of one episode\n","* Give an average episode return"]},{"cell_type":"code","metadata":{"id":"wiXJPDzauAvV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w0yEuu22vVDK"},"source":["# Subproject 2\n","\n","Solve [MoonLander-v2](https://gym.openai.com/envs/LunarLander-v2/) using DQN."]},{"cell_type":"markdown","metadata":{"id":"qWzbaAl3zlde"},"source":["**1. Import Necessary Packages:**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apijQg5Izmms","executionInfo":{"status":"ok","timestamp":1627370684295,"user_tz":-120,"elapsed":7146,"user":{"displayName":"Đorđe Grbić","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMtdPDPC6XJoAlVJ9SyzF_WldAaS1YlXbiUOAReA=s64","userId":"17224412897437983949"}},"outputId":"3c4e6d29-5f06-40fe-8047-900366ee662b"},"source":["!pip install box2d-py\n","#Imports\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import tensorflow as tf\n","from tensorflow import keras\n","#from keras.models import Sequential\n","#from keras.layers import Dense\n","#from keras.optimizers import Adam\n","import random\n","from gym import wrappers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement swig (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for swig\u001b[0m\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 4.4 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tBMyGxsqzwCd"},"source":["**2. Instantiate the Environment**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9dsd3yAz7Jp","executionInfo":{"status":"ok","timestamp":1627370689208,"user_tz":-120,"elapsed":390,"user":{"displayName":"Đorđe Grbić","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMtdPDPC6XJoAlVJ9SyzF_WldAaS1YlXbiUOAReA=s64","userId":"17224412897437983949"}},"outputId":"1d6fe8a5-9d2d-4c02-ff70-2523a3d6c12a"},"source":["env = gym.make('LunarLander-v2')\n","env.seed(0)\n","print('State shape: ', env.observation_space.shape)\n","print('Number of Actions: ', env.action_space.n)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["State shape:  (8,)\n","Number of Actions:  4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Paf8yGHfz--m"},"source":["**3. Implement and instantiate the agent**\n","\n"]},{"cell_type":"code","metadata":{"id":"kgmFtepK0G1d"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pw_aBoa40KkO"},"source":["**4. Train the agent with DQN**\n","\n","4.1 Show the episode return plot\n","  \n","  - Is the agent learning to solve the task?\n","\n","4.2 Save the best model"]},{"cell_type":"code","metadata":{"id":"wsG2JUqF0N_B"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JSVgVSMp0OOV"},"source":["**5. Load the model from the disk and run it in a loop**\n","- Hint: if you want to see the agent laning the Moon Lander, type `env.render()` after the `env.step()`.\n","- Do to Colab not cooperating with the Gym rendering, you might want to download the trained model and run this loop on you computer to visualise the behavior."]},{"cell_type":"markdown","metadata":{"id":"bQz2t49p1JHG"},"source":["**Helper functions**"]},{"cell_type":"markdown","metadata":{"id":"9jhtc8jF1XB2"},"source":["Save rendered images:"]},{"cell_type":"code","metadata":{"id":"Z_JgokDf1L0E"},"source":["import imageio\n","import numpy as np\n","\n","images = []\n","images.append(img)\n","img = model.env.render(mode='rgb_array')\n","\n","imageio.mimwrite('./moonlander.gif',\n","                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n","                fps=29)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wBRNoQ4X1heu"},"source":["Display saved .gif"]},{"cell_type":"code","metadata":{"id":"1J7b1mCL1km8"},"source":["from pathlib import Path\n","gifPath = Path(\"./moonlander.gif\")\n","# Display GIF in Jupyter, CoLab, IPython\n","with open(gifPath,'rb') as f:\n","    display.Image(data=f.read(), format='png')"],"execution_count":null,"outputs":[]}]}