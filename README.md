# Applied Artificial Intelligence (Summer University) (Spring 2021)

Three lectures per week.
Monday / Tuesday / Thursday / friday is question day
Wednesday and Fridays are dedicated to individual work and supervision at the University

<a href="https://colab.research.google.com/github/flight505/Applied_AI_IT_Uni/blob/main/" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


***
### day 01 Intro to the course and technology stack:
<b> The hand-ins for this session will be: </b>
- [x] 1b - Chipotle.

- [x] 7b - Numpy exercises.

- [x] 8b - Plotting exercises.


***
### day 02 Simple Regression Models:

<b> The hand-ins for this session will be: </b>

- [x] 02_PROJECT_linear-regression-house-price-prediction.ipynb



- [x] 02_Logistic_regression_heart_attack.ipynb

In this lecture, we will learn how to do a simple linear regression, reason about it and predict a continuous value based on on a set of variables. Furthermore, we will learn how to predict a categorical value using logistic regression. We will practice on datasets after the lecture.

notes: https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970

***
### day 03 Classification and Deep Learning:

<b> The hand-ins for this session will be: </b>
- [X] 03_fashion-mnist-with-keras-neural-networks.ipynb

Notes for Neural Networks:

https://drive.google.com/drive/folders/1M6gUQDsfNSCyH_dCuSqNdPSakj0FX2rt?usp=sharing

Exploring Fashion-MNIST with TensorFlow and Keras, https://medium.com/swlh/exploring-fashion-mnist-with-tensorflow-and-keras-aa780b766149


***

### day 04 Dimensionality Reduction and Clustering:

<b> The hand-ins for this session will be: </b>

- [x] PROJECT - Pizza - Dimensionality Reduction and Clustering.ipynb

The goal of this project is to check if you understand the PCA and KMeans algorithms that we covered in class.

We will walk you through computing the principal components of a matrix that shows the nutrient composition of pizzas of different brands.

***
### day 05 Introduction to NLP:

Text normalization using Gensim, we will load and preprocess the data from the six_thousand_tweets.csv file using the normalization techniques we talked about in the lecture.
next text processing
For this, we will end up covering several libraries, including (but not limited to):
- BeautifulSoup4.
- PyPDF2.
- urllib.
- NLTK.

No hand-ins for this day.
***

### day 06 word2vec and LDA:

<b> The hand-ins for this session will be: </b>

- [x] PROJECT - Speeches - Word Embeddings and Topic Modelling.ipynb
or
- [ ] ALTERNATIVE PROJECT - Word Embeddings and Topic Modelling.ipynb

  In this project, we will load and process the us_president_speeches.csv file, preprocess the speeches, embed their words and do topic modeling on them.

***
### day 07 Convolutional Neural Networks + Dropout and BatchNorm:

<b> The hand-ins for this session will be: </b>
- [ ] Project: PROJECT_Convolutional_networks.ipynb

Clarifications: Feel free to browse Kaggle, Tensorflow dataset database, or any other dataset. The Intel Image Classification dataset is an example dataset you might find interesting.

Avoid using MNIST-type dataset cause they are too easy to solve, pay attention that you don't take a massive dataset that will take forever to train on (like ImageNet). Also, pay attention that the dataset is not too small or has low-quality data points (too much noise, nonsense entries, etc).

***

### day 08 Reinforcement Learning (Deep Q-learning):

<b> The hand-ins for this session will be: </b>
- [ ] PROJECT_reinforcement_learning.ipynb


In this project we will solve two simple environments using a Q-table and a Neural Network (Deep Q-learning).

***
### day 09: Optimization (Gradient optimization, Population-based algorithms)

<b> The hand-ins for this session will be: </b>
- [ ] PROJECT_eggholder_and_tsp_optimization.ipynb

* The goal of this project is to optmize the Eggholder and Levi13 functions.
* Feel free to implement any of the algorithms we mentioned in the class
* Ideally, implement two of them and compare the results
* If two algorithms found the best solution, the better one is the one with fewer calls to the objective function

***
### day 10 Deep Generative Models:

<b> The hand-ins for this session will be: </b>
- [ ] Project - Deep Generative Models.ipynb

In this project we will explore three main things:
1. The relationship between linear autoencoders and PCA.
2. Training Generative Adversarial Networks.
3. Using already-trained GANs.


**Remember to activate GPUs in Colab**.



***
### day 11 Some discussions on AI Ethics:

In this class we will be discussing three topics:

A paper written by Bender & Gebru et al about the dangers of massive language models: https://dl.acm.org/doi/10.1145/3442188.3445922
A paper written by Buolamwini & Gebru about bias in computer vision systems: http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf
About the potential opacity and harm of the models we develop: O'Neil's concept of Weapons of Math Destruction. Here's a review: https://blogs.scientificamerican.com/roots-of-unity/review-weapons-of-math-destruction/

*NO HAND-IN*

***
### day 12 - Examination hand-ins for this course.

All projects must be handed in by noon on the 8Â´th August.

***



