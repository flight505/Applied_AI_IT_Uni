{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('AI_ITU': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Logistic_regression_heart_attack.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "529a06fb4068a4b13268933b910005f265dccc230859ef1ec378adb85153fb8f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "mDH2cSObPkh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first part of this exercise, we'll build a logistic regression model to predict whether a patient will going to get a cardiac arrest.  Suppose that you are a cardiologist and you want to determine each patient's chance of getting a heart attack based on the body measurments. You have historical data from previous patients that you can use as a training set for logistic regression.  To accomplish this, we're going to build a classification model that estimates the probability of admission based on the exam scores."
      ],
      "metadata": {
        "id": "8MniOK_UPkh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by examining the data."
      ],
      "metadata": {
        "id": "9a6ugGMoPkh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "##########################################################\n",
        "# Copyright (c) Jesper Vang <jesper_vang@me.com>         #\n",
        "# Created on 3 Aug 2021                                 #\n",
        "# Version:\t0.0.1                                        #\n",
        "# What:  \t\t\t\t\t\t                         #\n",
        "##########################################################\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "print(\"Pandas Version: {}\".format(pd.__version__))\n",
        "print(\"Numpy Version: {}\".format(np.__version__))\n",
        "print(\"Matplotlib Version: {}\".format(matplotlib.__version__))\n",
        "print(\"Scikit-learn Version: {}\".format(sklearn.__version__))\n",
        "print(\"Seaborn Version: {}\".format(sns.__version__))\n",
        "np.set_printoptions(suppress=True, linewidth=130)\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
        "\n",
        "print(f'the present working directory is: {os.getcwd()}')\n",
        "DATA_PATH = os.path.join(\"data\")\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "\n",
        "def load_model_data(data):\n",
        "    pwd = os.getcwd() \n",
        "    filepath = os.path.join(pwd, DATA_PATH, data) \n",
        "    return pd.read_csv(filepath)     \n",
        "    \n",
        "data = load_model_data(\"heart.csv\");\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "H_2gsHb6Pkh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Explore the data a bit.\n",
        "### List:\n",
        "\n",
        "1.   First few rows\n",
        "2.   Basic statistic\n",
        "3.   .info()\n",
        "4.   Column names\n"
      ],
      "metadata": {
        "id": "Tbohn8-5Pkh-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "features = np.shape(data)[0]\n",
        "samples = np.shape(data)[1]\n",
        "print(f\"Shape of Dataset: {samples} x {features}\\n\\t* Number of samples:\\t{samples}\\n\\t* Number of features:\\t{features}\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 1. First few rows\n",
        "data.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "uwE_hEmSPkh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "fb9492c5-364c-4ef9-a410-881b43e8acef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 2. Basic statistics\n",
        "data.describe()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "COMIuv17mBkM",
        "outputId": "26f3e721-996e-4b7f-93f5-56c35d2bb6cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 3. info()\n",
        "data.info()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRMnSJAemDXb",
        "outputId": "5d79e4b7-7260-4d66-baeb-96b3f385b84d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 4. Column names\n",
        "columns = data.columns.to_list()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoDDJK2mmE7y",
        "outputId": "ec3d0be0-1293-46c8-f399-bcdadc058420"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "\n",
        "1. What is the dependent variable (column name)?\n",
        "\n",
        "It is the 'target'.\n",
        "\n",
        "2. What are the independent variables?\n",
        "\n",
        "  The rest of the variables in the data set, \n",
        "\n",
        "  ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "\n",
        "3. Should we normalize the data?\n",
        "\n",
        "No, we don't need to normalize it.\n",
        "\n",
        "4. What are the column data-types?\n",
        "\n",
        "All columns are integers except for the \"oldpeak\", which is float. "
      ],
      "metadata": {
        "id": "F9yHU4e5njGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "Create some simple plots to check out the data!\n",
        "\n",
        "1.   Plot the pairwise scatter-plot between each column\n",
        "2.   Plot the distribution of the values of the dependent variable\n",
        "3.   Plot the pairwise correlation heatmap of each column.\n",
        "\n",
        "Answer questions:\n",
        "\n",
        "1.  What are the assumptions of the linear regression model?\n",
        "2.  Can we accept the basic assumptions of the linear regression model?\n",
        "3.  Judging by the scatter-plots, do you see any patterns in the data?\n",
        "4.  Judging by the correlation heat-map, is there correlation between the dependent variable and the independent variables?\n",
        "5.  Are there correlations among independent variables?"
      ],
      "metadata": {
        "id": "nIe7GZHRPkh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def draw_histograms(dataframe, features, rows, cols):\n",
        "    fig=plt.figure(figsize=(20,20))\n",
        "    for i, feature in enumerate(features):\n",
        "        ax=fig.add_subplot(rows,cols,i+1)\n",
        "        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n",
        "        ax.set_title(feature+\" Distribution\",color='DarkRed')\n",
        "        \n",
        "    fig.tight_layout()  \n",
        "    plt.show()\n",
        "#draw_histograms(data,data.columns,6,3)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# sns.pairplot(data=data)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to create a sigmoid function.\n",
        "\n",
        "Task:\n",
        "1.  Implement the function\n",
        "\n",
        "Make sure the function is correctly implemented.\n",
        "\n",
        "Task:\n",
        "2.  Plot the function."
      ],
      "metadata": {
        "id": "qwPjMiGOPkh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GNahZzz-Pkh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def sigmoid(z):\n",
        "# Activation function used to map any real value between 0 and 1\n",
        "    return 1/(1+ np.exp(-z))\n",
        "# test function    \n",
        "z = np.linspace(-10,10,num = 1000)\n",
        "fig = plt.figure(figsize = (5,2))\n",
        "sns.set(style = 'whitegrid')\n",
        "sns.lineplot(x = z, y = sigmoid(z))"
      ],
      "outputs": [],
      "metadata": {
        "id": "-7IIVc2KPkiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "18ea758d-5a3c-418e-d428-82fb6506679c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "1. Separate the data to `X` and `y` arrays.\n",
        "2. Separate the training set and evaluation set.\n",
        "3. Check the shape of our arrays to make sure everything looks good."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y = data[\"target\"] # The dependent variable is selected as y\n",
        "X = data.drop([\"target\"], axis=1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\" Shape: x_train {x_train.shape} x_test: {x_test.shape}\")\n",
        "print(f\" Shape: y_train {y_train.shape} y_test: {y_test.shape}\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "theta = [0.5]*len(X.columns)\n",
        "theta"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def hypothesis(theta, X):\n",
        "    z = np.dot(theta, X.T)\n",
        "    return 1/(1+np.exp(-(z))) - 0.0000001"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "1. Write the cost function to evaluate a solution."
      ],
      "metadata": {
        "id": "PgwrtQeHPkiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def cost(theta, X, y):\n",
        "    y1 = hypothesis(X, theta)\n",
        "    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "1. Compute the cost for our initial solution (eyeball the initial value, e.g. zero).\n",
        "2. Implement a function to compute the gradient (parameter updates) given our training data, labels, and model parameters."
      ],
      "metadata": {
        "id": "O0OcYsFzPkiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# number of elements\n",
        "nr_features = x_train.shape[1] # = 13\n",
        "\n",
        "nr = X.shape[0] # = 1000\n",
        "m = len(X) # = 303\n",
        "\n",
        "# initializing the theta values like 0.0. It can be initialized as for any other value\n",
        "init_theta = np.zeros(x_train.shape[1], dtype=float)  # initializing theta as zeroes\n",
        "\n",
        "# Learning Rate\n",
        "alpha = 0.00005\n",
        "# Number of iterations\n",
        "epochs = 200000\n",
        "\n",
        "def initial_solutions(init_theta, x_train, y_train):\n",
        "    cost_func_value = cost(init_theta, x_train, y_train)  # calculating the initial cost\n",
        "    print(f'The initial cost is: {cost_func_value:.2f}')\n",
        "\n",
        "initial_solutions(init_theta, x_train, y_train)\n",
        "#cost(init_theta, x_train, y_train)\n",
        "#len(hypothesis(x_train,init_theta))\n",
        "#len(sigmoid(np.dot(x_train, init_theta)))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def gradient_descent(theta, X, y, alpha, epochs):\n",
        "    m = len(X)\n",
        "    J = [cost(theta, X, y)]\n",
        "    for i in range(0, epochs):\n",
        "        if i % 25 == 0: \n",
        "        print('i = {}'.format(i))\n",
        "        h = hypothesis(X, theta)\n",
        "        for i in range(0, len(X.columns)):\n",
        "            theta[i] -= (alpha / m) * np.sum((h - y) * X.iloc[:, i])\n",
        "        J.append(cost(theta, X, y))\n",
        "    return J, theta\n",
        "\n",
        "\n",
        "# gradient_descent(init_theta, x_train, y_train, alpha, epochs)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "L, theta = gradient_descent(init_theta, x_train, y_train, alpha, epochs)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def predict(theta, X, y, alpha, epochss):\n",
        "    J, th = gradient_descent(theta, X, y, alpha, epochs) \n",
        "    h = hypothesis(X, theta)\n",
        "    for i in range(len(h)):\n",
        "        h[i]=1 if h[i]>=0.5 else 0\n",
        "    y = list(y)\n",
        "    acc = np.sum([y[i] == h[i] for i in range(len(y))])/len(y)\n",
        "    return J, acc"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "L, theta = gradient_descent(init_theta, x_train, y_train, alpha, epochs)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predict(init_theta, x_train, y_train, alpha, epochs)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "1. Calculate the cost for the optimized parameters"
      ],
      "metadata": {
        "id": "dU5R2kiMPkiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "theta = [0.5]*len(X.columns)\n",
        "J, acc = predict(init_theta, x_train, y_train, alpha, epochs)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "acc"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.scatter(range(0, len(J)), J)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(x_train, y_train)\n",
        "y_pred = logreg.predict(x_test)\n",
        "print(f\"Accuracy of logistic regression classifier on test set: {logreg.score(x_test, y_test):.2f}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "7Gfw8lHOPkiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3e245d-f58a-4402-8c97-a4df0222091f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(logreg.coef_)\n",
        "cost(logreg.coef_[0], x_train, y_train)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tozqLcLYi6Sr",
        "outputId": "568e3bbb-2ead-4925-bde2-7d8daada8aaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "1. Write a function that will output predictions for a dataset X using our learned parameters.\n",
        "2. Use this function to score and print the training accuracy of our classifier."
      ],
      "metadata": {
        "id": "bPXoNDAtPkiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def predict(x, theta):\n",
        "  return sigmoid(np.dot(x, theta))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Yfx_Iuj6PkiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Hint: Accuracy is calculated  like: correctly classified samples / all samples\n",
        "pred = np.round(predict(x_test, updated_weights))\n",
        "accuracy = (y_test == pred).sum() / float(len(y_test))\n",
        "print(f\"The accuracy is: {accuracy}\")\n",
        "\n",
        "# Hint: Accuracy is calculated  like: correctly classified samples / all samples\n",
        "\n",
        "pred = np.round(predict(x_test, updated_weights))\n",
        "\n",
        "# I find the accuracy\n",
        "accuracy = (y_test == pred).sum() / float(len(y_test))\n",
        "\n",
        "print(\"The accuracy is: \", accuracy)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wGMPM_ImPkiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9f323c-e8fb-43bf-940d-1b106d732ad0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.scatter(X_test.index,X_test.values,c=y_predict_test)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}